# Colab-optimized configuration
# Use with: python3 scripts/train.py --config config/colab.yaml

experiment:
  name: "colab_training"
  description: "Optimized for Colab T4 GPU (16GB VRAM)"
  version: "v1.0"

paths:
  coco_root: "dataset/coco_train2017/"
  checkpoint_dir: "checkpoints/colab"
  log_dir: "logs/"
  superpoint_weights: "dataset/superpoint_v1.pth"

system:
  device: "cuda"
  num_workers: 2  # Colab has limited CPU
  seed: 42

model:
  teacher: "alike"
  alike_model: "alike-t"
  descriptor_dim: 64
  width_mult: 1.0
  use_depthwise: true
  use_hadamard: true

training:
  batch_size: 16  # T4 can handle larger batches
  gradient_accumulation_steps: 1  # No accumulation needed
  epochs: 15
  n_steps: 100000  # Shorter for Colab time limits
  lr: 0.0003
  save_interval: 5000

  loss_weights:
    heatmap: 1.0
    distill: 1.0
    fine: 1.0
    reliability: 1.0

  scheduler:
    conf_thresh:
      start_val: 0.0
      end_val: 0.15
      start_pct: 0.0
      end_pct: 0.5

visualization:
  colormap: "hot"

augmentation:
  warp_resolution: [640, 480]
  out_resolution: [640, 480]
  sides_crop: 0.1
  photometric: true
  geometric: true
  use_tps: true
  tps_prob: 0.5
  difficulty: 0.3
