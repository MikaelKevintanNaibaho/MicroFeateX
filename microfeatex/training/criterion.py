import torch
import torch.nn as nn
import torch.nn.functional as F
from microfeatex.training import losses


class MicroFeatEXCriterion(nn.Module):
    """
    Encapsulates the Distillation and Matching Loss logic.
    Decouples the Teacher type from the main training loop.
    """

    def __init__(self, config: dict, device: str):
        super().__init__()
        self.config = config
        self.device = device

        # Loss Weights
        loss_cfg = config.get("training", {}).get("loss_weights", {})
        self.w_heatmap = loss_cfg.get("heatmap", 10.0)
        self.w_distill = loss_cfg.get("distill", 1.0)
        self.w_reliability = loss_cfg.get("reliability", 0.1)
        self.w_fine = loss_cfg.get("fine", 1.0)

        # Determine Teacher Type
        self.teacher_type = config.get("model", {}).get("teacher", "superpoint").lower()

    def _process_superpoint_teacher(self, scores, shape):
        """Converting SP logits to heatmap for loss calculation."""
        H, W = shape
        h_grid, w_grid = H // 8, W // 8

        # Handle potential transpose
        if scores.shape[-1] == H and scores.shape[-2] == W:
            scores = scores.transpose(-1, -2)

        # Force resize to coarse grid if mismatch
        if scores.shape[-2] != h_grid or scores.shape[-1] != w_grid:
            scores = F.interpolate(
                scores, size=(h_grid, w_grid), mode="bilinear", align_corners=False
            )
        return scores

    def compute_distillation(self, student_out, teacher_out, input_shape):
        """Computes the Heatmap/Distillation loss depending on teacher type."""
        s_logits = student_out["keypoint_logits"]  # [B, 65, H/8, W/8]

        if self.teacher_type == "superpoint":
            # SuperPoint Teacher returns logits
            # We assume teacher_out has "scores" (the logits) and "heatmap" (processed)
            # If "heatmap" is missing, we must generate it, but the wrapper usually handles it.

            # Use the specific SuperPoint distillation loss
            loss, acc = losses.batch_alike_distill_loss(
                s_logits, teacher_out["heatmap"], grid_size=8
            )

        elif self.teacher_type == "alike":
            # ALIKE Teacher returns a Heatmap directly
            t_heatmap = teacher_out["heatmap"]
            loss, acc = losses.batch_alike_distill_loss(
                s_logits, t_heatmap, grid_size=8
            )
        else:
            raise ValueError(f"Unknown teacher type: {self.teacher_type}")

        return loss, acc

    def forward(self, student_out, teacher_out, batch_data):
        """
        Calculates total distillation loss.
        """
        # Unpack batch info (generated by augmentor)
        # batch_data structure: (p1, p2, H1, H2)
        p1, p2, _, _ = batch_data

        # Unpack outputs (tuples of results for img1 and img2)
        s_out1, s_out2 = student_out
        t_out1, t_out2 = teacher_out

        # 1. Distillation Loss (Heatmap)
        loss_distill_1, acc_1 = self.compute_distillation(s_out1, t_out1, p1.shape[2:])
        loss_distill_2, acc_2 = self.compute_distillation(s_out2, t_out2, p2.shape[2:])

        loss_heatmap = (loss_distill_1 + loss_distill_2) / 2.0
        acc_heatmap = (acc_1 + acc_2) / 2.0

        return {"loss_heatmap": loss_heatmap, "acc_heatmap": acc_heatmap}
